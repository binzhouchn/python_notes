{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Program Files\\Anaconda3\\lib\\site-packages\\gensim\\utils.py:1167: UserWarning: detected Windows; aliasing chunkize to chunkize_serial\n",
      "  warnings.warn(\"detected Windows; aliasing chunkize to chunkize_serial\")\n",
      "C:\\Program Files\\Anaconda3\\lib\\site-packages\\sklearn\\cross_validation.py:41: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x599ea70>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.utils.data as Data\n",
    "import torchvision      # 数据库模块\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import gensim\n",
    "import jieba\n",
    "import jieba.posseg as pseg\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from gensim.models import Word2Vec\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.metrics import accuracy_score\n",
    "import matplotlib.pyplot as plt\n",
    "torch.manual_seed(1)    # reproducible"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. 加载数据，处理数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data = pd.read_csv('data/zhishiku.csv',encoding='utf-8',sep='\\t')\n",
    "data['label'] = LabelEncoder().fit_transform(data.categ_id)\n",
    "data = data[['categ_id','standard_question']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 对label进行数值化处理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data['label'] = LabelEncoder().fit_transform(data.categ_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 对X进行数值化处理 用之前训练好的词向量"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# 如果词不在词向量中则用0代替\n",
    "UNK = np.zeros(400)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 用jieba切词，词的长度少于6个则都用不足的用UNK补上，如果大于6个则选n和v，选完后如果少于6个则用UNK代替，如果还多于6个则取前6个"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from the default dictionary ...\n",
      "Loading model from cache C:\\Users\\16121360\\AppData\\Local\\Temp\\jieba.cache\n",
      "Loading model cost 0.973 seconds.\n",
      "Prefix dict has been built succesfully.\n"
     ]
    }
   ],
   "source": [
    "jieba.load_userdict('data/user_dict.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# 加载停用词\n",
    "f = open('data/stop_words.txt','r',encoding='utf-8')\n",
    "stop_words_list = [line.strip() for line in f.readlines()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 词的长度少于6个则都用不足的用UNK补上，如果大于等于6个则选n和v;选完后如果少于6个再选m,t,a的如果选完还是不足则用UNK代替\n",
    "def ff(s):\n",
    "    pg = pseg.lcut(s)\n",
    "    pg = [x for x in pg if x.word not in stop_words_list] # 去停用词\n",
    "    pg_l = len(pg)\n",
    "    if pg_l < 7:\n",
    "        return [x.word for x in pg if x.word not in stop_words_list] + ['UNK']*(6-pg_l)\n",
    "    else:\n",
    "        pg2 = [x for x in pg if x.flag in ('n','nr','ns','nt','nz','v','vn','vd','vg')][:6]\n",
    "        pg2_l = len(pg2)\n",
    "        if pg2_l < 6:\n",
    "            pg2 += [x for x in pg if x.flag in ('m','t','a')][:6-pg2_l]\n",
    "        return [x.word for x in pg2 if x.word not in stop_words_list] + ['UNK']*(6-len(pg2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## 分期付款用jieba怎么都分不开。。。手工分下\n",
    "def ff2(l):\n",
    "    if '分期付款' in l:\n",
    "        idx = l.index('分期付款')\n",
    "        l.pop(idx)\n",
    "        l.insert(idx,'分期')\n",
    "        l.insert(idx+1,'付款')\n",
    "        l.pop(-1)\n",
    "        return l\n",
    "    return l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 5.85 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "pg3 = data.standard_question.apply(ff)\n",
    "pg3 = pg3.apply(ff2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 加载word2vec模型\n",
    "model=Word2Vec.load('model/skip_dia.model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# 对X进行数值化处理，拿出词向量，没有的用0填充\n",
    "data['X'] = pg3.apply(lambda x : np.array([model.wv[s] if s in model.wv.vocab.keys() else UNK for s in x]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# 保存dataframe，用csv保存方法发现有点问题\n",
    "# from sklearn.externals import joblib #jbolib模块\n",
    "# joblib.dump(data[['label','X']], 'data/all_numerical_data.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def train_test_sep(X, test_size = 0.3, stratify = None, random_state = 1001):\n",
    "        train, test = train_test_split(X, test_size = test_size, stratify = stratify, random_state = random_state)\n",
    "        return train, test\n",
    "train1, test = train_test_sep(data[['label','X']])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4 统计一下训练数据每个类别的count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>X</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>label</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1258</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>890</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>117</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>66</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>288</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>92</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>574</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>461</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>198</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>73</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>185</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          X\n",
       "label      \n",
       "0      1258\n",
       "1       890\n",
       "2       117\n",
       "3        66\n",
       "4       288\n",
       "5        92\n",
       "6       574\n",
       "7       461\n",
       "8       198\n",
       "9        73\n",
       "10      185"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train1.groupby(by='label').count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.5 对类别count少的进行上采样 train1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n类别2*2倍\\n类别3*4倍\\n类别5*2倍\\n类别8*1倍\\n类别9*3倍\\n类别10*1倍\\n'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "类别2*2倍\n",
    "类别3*4倍\n",
    "类别5*2倍\n",
    "类别8*1倍\n",
    "类别9*3倍\n",
    "类别10*1倍\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 进行上采样\n",
    "train = pd.concat([train1,train1[train1.label==2],train1[train1.label==2],train1[train1.label==3],train1[train1.label==3],\\\n",
    "                  train1[train1.label==3],train1[train1.label==3],train1[train1.label==5],train1[train1.label==5],\\\n",
    "                  train1[train1.label==8],train1[train1.label==9],train1[train1.label==9],train1[train1.label==9],\n",
    "                  train1[train1.label==10]],axis=0)\n",
    "# 不进行上采样\n",
    "train = train1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 进行一步shuffle操作，打乱顺序对训练效果会好一点\n",
    "train = shuffle(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>X</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3859</th>\n",
       "      <td>6</td>\n",
       "      <td>[[-0.18964225053787231, 0.13085795938968658, 0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3224</th>\n",
       "      <td>6</td>\n",
       "      <td>[[-0.4948671758174896, -0.07261338084936142, -...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4303</th>\n",
       "      <td>8</td>\n",
       "      <td>[[-0.12191753089427948, 0.32593342661857605, 0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51</th>\n",
       "      <td>4</td>\n",
       "      <td>[[-0.33129289746284485, 0.2676745057106018, -0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1429</th>\n",
       "      <td>0</td>\n",
       "      <td>[[-0.155685693025589, -0.08552972972393036, 0....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>622</th>\n",
       "      <td>0</td>\n",
       "      <td>[[-0.04397333785891533, -0.028153445571660995,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4373</th>\n",
       "      <td>8</td>\n",
       "      <td>[[-0.04569557, 0.10023516, 0.09676585, 0.02597...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4615</th>\n",
       "      <td>9</td>\n",
       "      <td>[[-0.24380038678646088, 0.39492788910865784, 0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5318</th>\n",
       "      <td>7</td>\n",
       "      <td>[[-0.05486312508583069, -0.05952757969498634, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3391</th>\n",
       "      <td>6</td>\n",
       "      <td>[[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      label                                                  X\n",
       "3859      6  [[-0.18964225053787231, 0.13085795938968658, 0...\n",
       "3224      6  [[-0.4948671758174896, -0.07261338084936142, -...\n",
       "4303      8  [[-0.12191753089427948, 0.32593342661857605, 0...\n",
       "51        4  [[-0.33129289746284485, 0.2676745057106018, -0...\n",
       "1429      0  [[-0.155685693025589, -0.08552972972393036, 0....\n",
       "622       0  [[-0.04397333785891533, -0.028153445571660995,...\n",
       "4373      8  [[-0.04569557, 0.10023516, 0.09676585, 0.02597...\n",
       "4615      9  [[-0.24380038678646088, 0.39492788910865784, 0...\n",
       "5318      7  [[-0.05486312508583069, -0.05952757969498634, ...\n",
       "3391      6  [[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,..."
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 查看下数据结构\n",
    "train.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. 跑CNN神经网络 Conv2D"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 数据处理成tensor，dataset，dataloader形式"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "EPOCH = 3           # 训练整批数据多少次, 为了节约时间, 我们只训练三次\n",
    "BATCH_SIZE = 64\n",
    "LR = 0.003          # 学习率"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "conv2d training data\n",
      "torch.Size([4202, 6, 400])\n",
      "torch.Size([4202, 1, 6, 400])\n",
      "torch.Size([4202])\n",
      "\n",
      "\n",
      "conv2d testing data\n",
      "torch.Size([1801, 6, 400])\n",
      "torch.Size([1801, 1, 6, 400])\n",
      "torch.Size([1801])\n"
     ]
    }
   ],
   "source": [
    "# 这里trainX_tensor转成trainX是为了加一个维度，值为1；\n",
    "# 本来一句话是6*400，现在变成1*6*400；如果是彩色图片有RGB，值为3\n",
    "print('conv2d training data')\n",
    "trainX_tensor = torch.from_numpy(np.array([x for x in train.X]))\n",
    "print(trainX_tensor.shape)\n",
    "trainX = torch.unsqueeze(trainX_tensor, dim=1).type(torch.FloatTensor)\n",
    "print(trainX.shape)\n",
    "trainy_tensor = torch.from_numpy(np.array(train.label))\n",
    "print(trainy_tensor.shape)\n",
    "print('\\n')\n",
    "#--------------------------------\n",
    "print('conv2d testing data')\n",
    "testX_tensor = torch.from_numpy(np.array([x for x in test.X]))\n",
    "print(testX_tensor.shape)\n",
    "testX = torch.unsqueeze(testX_tensor, dim=1).type(torch.FloatTensor)\n",
    "print(testX.shape)\n",
    "testy_tensor = torch.from_numpy(np.array(test.label))\n",
    "print(testy_tensor.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 组装成dataset，到时候放入dataloader(放入dataloader是为了进行批训练)\n",
    "torch_dataset = Data.TensorDataset(trainX, trainy_tensor)\n",
    "train_loader = Data.DataLoader(\n",
    "    dataset=torch_dataset,      # torch TensorDataset format\n",
    "    batch_size=BATCH_SIZE,      # mini batch size\n",
    "    shuffle=True,               # random shuffle for training\n",
    "    num_workers=2,              # subprocesses for loading data\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 构建cnn模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CNN(\n",
      "  (conv1): Sequential(\n",
      "    (0): Conv2d(1, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (1): ReLU()\n",
      "    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      "  (conv2): Sequential(\n",
      "    (0): Conv2d(16, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (1): ReLU()\n",
      "    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      "  (out): Linear(in_features=3200, out_features=11, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "class CNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CNN, self).__init__()\n",
    "        self.conv1 = nn.Sequential(  # input shape (1, 6, 400) 如果加上样本量那就是(4202, 1, 6, 400)\n",
    "            nn.Conv2d(\n",
    "                in_channels=1,      # input height 又叫通道\n",
    "                out_channels=16,    # n_filters output height\n",
    "                kernel_size=3,      # filter size\n",
    "                stride=1,           # filter movement/step\n",
    "                padding=1,      # 如果想要 con2d 出来的图片长宽没有变化, padding=(kernel_size-1)/2 当 stride=1\n",
    "            ),      # output shape (16, 6, 400)\n",
    "            nn.ReLU(),    # activation\n",
    "            nn.MaxPool2d(kernel_size=2),    # 在 2x2 空间里向下采样, output shape (16, 3, 200)\n",
    "        )\n",
    "        self.conv2 = nn.Sequential(  # input shape (16, 3, 200)\n",
    "            nn.Conv2d(16, 32, 3, 1, 1),  # output shape (32, 3, 200)\n",
    "            nn.ReLU(),  # activation\n",
    "            nn.MaxPool2d(2),  # output shape (32, 1, 100)\n",
    "        )\n",
    "        self.out = nn.Linear(32 * 1 * 100, 11)   # fully connected layer, output 11 classes\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.conv2(x)\n",
    "        x = x.view(x.size(0), -1)   # 展平多维的卷积图成 (batch_size, 32 * 1 * 100)\n",
    "        output = self.out(x)\n",
    "        return output\n",
    "\n",
    "cnn = CNN()\n",
    "print(cnn)  # net architecture"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(cnn.parameters(), lr=LR)   # optimize all cnn parameters;Adam比较好用\n",
    "loss_func = nn.CrossEntropyLoss()   # the target label is not one-hotted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 19.4 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# training and testing\n",
    "for epoch in range(EPOCH):\n",
    "    for step, (b_x, b_y) in enumerate(train_loader):   # 分配 batch data, normalize x when iterate train_loader\n",
    "        output = cnn(b_x)               # cnn output\n",
    "        loss = loss_func(output, b_y)   # cross entropy loss\n",
    "        optimizer.zero_grad()           # clear gradients for this training step\n",
    "        loss.backward()                 # backpropagation, compute gradients\n",
    "        optimizer.step()                # apply gradients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_output = cnn(testX)\n",
    "pred_y = torch.max(test_output, 1)[1].data.numpy().squeeze()\n",
    "# 评价准确率\n",
    "accuracy_score(testy_tensor.numpy(),pred_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. 保存模型"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 方法一"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Program Files\\Anaconda3\\lib\\site-packages\\torch\\serialization.py:193: UserWarning: Couldn't retrieve source code for container of type CNN. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n"
     ]
    }
   ],
   "source": [
    "# 保存\n",
    "torch.save(cnn, 'cnn1.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 加载\n",
    "cnn = torch.load('cnn1.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 方法二（推荐）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 保存\n",
    "torch.save(cnn.state_dict(), 'cnn1_params.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 加载\n",
    "class CNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CNN, self).__init__()\n",
    "        self.conv1 = nn.Sequential(  # input shape (1, 6, 400)\n",
    "            nn.Conv2d(\n",
    "                in_channels=1,      # input height\n",
    "                out_channels=16,    # n_filters\n",
    "                kernel_size=3,      # filter size\n",
    "                stride=1,           # filter movement/step\n",
    "                padding=1,      # 如果想要 con2d 出来的图片长宽没有变化, padding=(kernel_size-1)/2 当 stride=1\n",
    "            ),      # output shape (16, 6, 400)\n",
    "            nn.ReLU(),    # activation\n",
    "            nn.MaxPool2d(kernel_size=2),    # 在 2x2 空间里向下采样, output shape (16, 3, 200)\n",
    "        )\n",
    "        self.conv2 = nn.Sequential(  # input shape (16, 3, 200)\n",
    "            nn.Conv2d(16, 32, 3, 1, 1),  # output shape (32, 3, 200)\n",
    "            nn.ReLU(),  # activation\n",
    "            nn.MaxPool2d(2),  # output shape (32, 1, 100)\n",
    "        )\n",
    "        self.out = nn.Linear(32 * 1 * 100, 11)   # fully connected layer, output 11 classes\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.conv2(x)\n",
    "        x = x.view(x.size(0), -1)   # 展平多维的卷积图成 (batch_size, 32 * 2 * 100)\n",
    "        output = self.out(x)\n",
    "        return output\n",
    "\n",
    "cnn = CNN()\n",
    "# 定义完和save前一样的cnn结构以后，加载模型参数\n",
    "cnn.load_state_dict(torch.load('cnn1_params.pkl'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#############"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2(2). 跑cnn模型之用Conv1D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "conv1d training data\n",
      "torch.Size([4202, 6, 400])\n",
      "torch.Size([4202])\n",
      "\n",
      "\n",
      "conv1d testing data\n",
      "torch.Size([1801, 6, 400])\n",
      "torch.Size([1801])\n"
     ]
    }
   ],
   "source": [
    "# 注意conv1d不需要conv2d那样增加channel，conv1d没有height；\n",
    "# 还有这里的trainX要变成float，trainy要变成long\n",
    "print('conv1d training data')\n",
    "trainX = torch.from_numpy(np.array([x for x in train.X])).type(torch.DoubleTensor).float() # .float()\n",
    "print(trainX.shape)\n",
    "trainy_tensor = torch.from_numpy(np.array(train.label)).type(torch.DoubleTensor).long() # .long()\n",
    "print(trainy_tensor.shape) \n",
    "print('\\n')\n",
    "#--------------------------------\n",
    "print('conv1d testing data')\n",
    "testX = torch.from_numpy(np.array([x for x in test.X])).type(torch.DoubleTensor).float()\n",
    "print(testX.shape)\n",
    "testy_tensor = torch.from_numpy(np.array(test.label)).type(torch.DoubleTensor).long()\n",
    "print(testy_tensor.shape)\n",
    "\n",
    "# 组装成dataset，到时候放入dataloader(放入dataloader是为了进行批训练)\n",
    "torch_dataset = Data.TensorDataset(trainX, trainy_tensor)\n",
    "train_loader = Data.DataLoader(\n",
    "    dataset=torch_dataset,      # torch TensorDataset format\n",
    "    batch_size=BATCH_SIZE,      # mini batch size\n",
    "    shuffle=True,               # random shuffle for training\n",
    "    num_workers=2,              # subprocesses for loading data\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 构建cnn模型 conv1d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CNN(\n",
      "  (conv1): Sequential(\n",
      "    (0): Conv1d(6, 16, kernel_size=(3,), stride=(1,), padding=(1,))\n",
      "    (1): ReLU()\n",
      "    (2): MaxPool1d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      "  (conv2): Sequential(\n",
      "    (0): Conv1d(16, 32, kernel_size=(3,), stride=(1,), padding=(1,))\n",
      "    (1): ReLU()\n",
      "    (2): MaxPool1d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      "  (out): Linear(in_features=3200, out_features=11, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "class CNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CNN, self).__init__()\n",
    "        self.conv1 = nn.Sequential(  # input shape (6, 400) 样本数不算，算上的话就是(4202, 6, 400)\n",
    "            nn.Conv1d(\n",
    "                in_channels=6,      # 这里相当于词的个数，固定句子中的词都为6\n",
    "                out_channels=16,    # 输出的词，也相当于特征\n",
    "                kernel_size=3,      # filter size ；conv1d中筛子维度是1*3\n",
    "                stride=1,           # filter movement/step\n",
    "                padding=1,      # 如果想要 conv1d 出来的图片长宽没有变化, padding=(kernel_size-1)/2 当 stride=1\n",
    "            ),      # output shape (16, 400)\n",
    "            nn.ReLU(),    # activation\n",
    "            nn.MaxPool1d(kernel_size=2),    # 在 1x2 空间里向下采样, output shape (16, 200)\n",
    "        )\n",
    "        self.conv2 = nn.Sequential(  # input shape (16, 200)\n",
    "            nn.Conv1d(16, 32, 3, 1, 1),  # output shape (32, 200)\n",
    "            nn.ReLU(),  # activation\n",
    "            nn.MaxPool1d(2),  # output shape (32, 100)\n",
    "        )\n",
    "        self.out = nn.Linear(32 * 100, 11)   # fully connected layer, output 11 classes\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.conv2(x)\n",
    "        x = x.view(x.size(0), -1)   # 展平多维的卷积图成 (batch_size, 32 * 100)\n",
    "        output = self.out(x)\n",
    "        return output\n",
    "\n",
    "cnn = CNN()\n",
    "print(cnn)  # net architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(cnn.parameters(), lr=LR)   # optimize all cnn parameters;Adam比较好用\n",
    "loss_func = nn.CrossEntropyLoss()   # the target label is not one-hotted\n",
    "# training and testing\n",
    "for epoch in range(EPOCH):\n",
    "    for step, (b_x, b_y) in enumerate(train_loader):   # 分配 batch data, normalize x when iterate train_loader\n",
    "        output = cnn(b_x)               # cnn output\n",
    "        loss = loss_func(output, b_y)   # cross entropy loss\n",
    "        optimizer.zero_grad()           # clear gradients for this training step\n",
    "        loss.backward()                 # backpropagation, compute gradients\n",
    "        optimizer.step()                # apply gradients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9722376457523598"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_output = cnn(testX)\n",
    "pred_y = torch.max(test_output, 1)[1].data.numpy().squeeze()\n",
    "# 评价准确率\n",
    "accuracy_score(testy_tensor.numpy(),pred_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "##########"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
