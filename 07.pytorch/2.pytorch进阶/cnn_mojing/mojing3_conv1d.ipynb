{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/anaconda3/lib/python3.6/site-packages/sklearn/cross_validation.py:41: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7fb205a5e7b0>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.utils.data as Data\n",
    "import torchvision      # 数据库模块\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from gensim.models import Word2Vec\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import log_loss\n",
    "torch.manual_seed(1)    # reproducible"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.构建embedding矩阵 train和test要合起来"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "class BOW(object):\n",
    "    def __init__(self, X, min_count=10, maxlen=100):\n",
    "        \"\"\"\n",
    "        X: [[w1, w2],]]\n",
    "        \"\"\"\n",
    "        self.X = X\n",
    "        self.min_count = min_count\n",
    "        self.maxlen = maxlen\n",
    "        self.__word_count()\n",
    "        self.__idx()\n",
    "        self.__doc2num()\n",
    "\n",
    "    def __word_count(self):\n",
    "        wc = {}\n",
    "        for ws in tqdm(self.X, desc='   Word Count'):\n",
    "            for w in ws:\n",
    "                if w in wc:\n",
    "                    wc[w] += 1\n",
    "                else:\n",
    "                    wc[w] = 1\n",
    "        self.word_count = {i: j for i, j in wc.items() if j >= self.min_count}\n",
    "\n",
    "    def __idx(self):\n",
    "        self.idx2word = {i + 1: j for i, j in enumerate(self.word_count)}\n",
    "        self.word2idx = {j: i for i, j in self.idx2word.items()}\n",
    "\n",
    "    def __doc2num(self):\n",
    "        doc2num = []\n",
    "        for text in tqdm(self.X, desc='Doc To Number'):\n",
    "            s = [self.word2idx.get(i, 0) for i in text[:self.maxlen]]\n",
    "            doc2num.append(s + [0]*(self.maxlen-len(s)))  # 未登录词全部用0表示\n",
    "        self.doc2num = np.asarray(doc2num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_ids(qids):\n",
    "    ids = []\n",
    "    for t_ in qids:\n",
    "        ids.append(int(t_[1:]))\n",
    "    return np.asarray(ids)\n",
    "\n",
    "\n",
    "def get_texts(file_path, question_path):\n",
    "    qes = pd.read_csv(question_path)\n",
    "    file = pd.read_csv(file_path)\n",
    "    q1id, q2id = file['q1'], file['q2']\n",
    "    id1s, id2s = get_ids(q1id), get_ids(q2id)\n",
    "    all_words = qes['words']\n",
    "    texts = []\n",
    "    for t_ in zip(id1s, id2s):\n",
    "        texts.append(all_words[t_[0]] + ' ' + all_words[t_[1]])\n",
    "    return texts\n",
    "TRAIN_PATH = 'mojing/train.csv'\n",
    "TEST_PATH = 'mojing/test.csv'\n",
    "QUESTION_PATH = 'mojing/question.csv'\n",
    "train_texts = get_texts(TRAIN_PATH, QUESTION_PATH)\n",
    "test_texts = get_texts(TEST_PATH, QUESTION_PATH)\n",
    "a = train_texts + test_texts\n",
    "a1 = [x.split(' ') for x in a]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "   Word Count: 100%|██████████| 427342/427342 [00:01<00:00, 348343.04it/s]\n",
      "Doc To Number: 100%|██████████| 427342/427342 [00:02<00:00, 155920.04it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 4.75 s, sys: 97.2 ms, total: 4.84 s\n",
      "Wall time: 4.82 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "bow = BOW(a1,min_count=1,maxlen=24) # count大于1，句子(q1,q2)相加最大长度为24"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 得到词向量"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2.43 s, sys: 184 ms, total: 2.62 s\n",
      "Wall time: 3.44 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "word_embed = pd.read_csv('mojing/word_embed.txt',header=None)\n",
    "word_embed.columns = ['wv']\n",
    "word_embed_dict = dict()\n",
    "for s in word_embed.wv.values:\n",
    "    l = s.split(' ')\n",
    "    word_embed_dict[l[0]] = list(map(float,l[1:]))\n",
    "word_embed_dict['UNK'] = [0]*300"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = len(word_embed_dict)\n",
    "embedding_matrix = np.zeros((vocab_size+1,300))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "for key, value in bow.word2idx.items():\n",
    "    embedding_matrix[value] = word_embed_dict.get(key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.        ,  0.        ,  0.        , ...,  0.        ,\n",
       "         0.        ,  0.        ],\n",
       "       [ 2.29952765, -4.29687977,  3.71340919, ...,  0.99011242,\n",
       "         0.41728863,  3.15365911],\n",
       "       [-1.52279055,  2.12538552, -0.3590863 , ..., -2.17771411,\n",
       "         1.37241161, -3.44047666],\n",
       "       ...,\n",
       "       [ 0.        ,  0.        ,  0.        , ...,  0.        ,\n",
       "         0.        ,  0.        ],\n",
       "       [ 0.        ,  0.        ,  0.        , ...,  0.        ,\n",
       "         0.        ,  0.        ],\n",
       "       [ 0.        ,  0.        ,  0.        , ...,  0.        ,\n",
       "         0.        ,  0.        ]])"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# np.save('save/embedding_matrix.npz',embedding_matrix)\n",
    "# embedding_matrix = np.load('save/embedding_matrix.npz.npy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. 导入数据，处理数据"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 拼接数据 将q1和q2问题的词拼接起来，做cnn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "## save\n",
    "# train[['label','words']].to_csv('mojing1/train_words2.csv',index=False)\n",
    "# test[['words']].to_csv('mojing1/test_words2.csv',index=False)\n",
    "# load\n",
    "train = pd.read_csv('mojing1/train_words2.csv')\n",
    "test = pd.read_csv('mojing1/test_words2.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fill_unkQ1(s):\n",
    "    l1 = s.split(',')[0].split(' ')[:12]\n",
    "    l1 += ['UNK']*(12-len(l1))\n",
    "    l1 = [bow.word2idx.get(x) if x in bow.word2idx.keys() else 0 for x in l1]\n",
    "    return l1\n",
    "def fill_unkQ2(s):\n",
    "    l2 = s.split(',')[1].split(' ')[:12]\n",
    "    l2 += ['UNK']*(12-len(l2))\n",
    "    l2 = [bow.word2idx.get(x) if x in bow.word2idx.keys() else 0 for x in l2]\n",
    "    return l2\n",
    "# train = train.words.apply(lambda x : x.split(',')[0].split(' ')[:12]).tolist()\n",
    "# test = test.words.apply(lambda x : x.split(',')[0].split(' ')[:12]).tolist()\n",
    "train_q1 = train.words.apply(fill_unkQ1).tolist()\n",
    "train_q2 = train.words.apply(fill_unkQ2).tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3.构建CNN模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from BasicModule import BasicModule\n",
    "import torch as t\n",
    "import numpy as np\n",
    "from torch import nn\n",
    "\n",
    "kernel_sizes =  [1,2,3,4]\n",
    "kernel_sizes2 = [1,2,3,4]\n",
    "class MultiCNNTextBNDeep(BasicModule): \n",
    "    def __init__(self, opt ):\n",
    "        super(MultiCNNTextBNDeep, self).__init__()\n",
    "        self.model_name = 'MultiCNNTextBNDeep'\n",
    "        self.opt=opt\n",
    "        self.encoder = nn.Embedding(opt.vocab_size,opt.embedding_dim)\n",
    "\n",
    "        title_convs = [ nn.Sequential(\n",
    "                                nn.Conv1d(in_channels = opt.embedding_dim,\n",
    "                                        out_channels = opt.title_dim,\n",
    "                                        kernel_size = kernel_size),\n",
    "                                nn.BatchNorm1d(opt.title_dim),\n",
    "                                nn.ReLU(inplace=True),\n",
    "\n",
    "                                nn.Conv1d(in_channels = opt.title_dim,\n",
    "                                out_channels = opt.title_dim,\n",
    "                                kernel_size = kernel_size),\n",
    "                                nn.BatchNorm1d(opt.title_dim),\n",
    "                                nn.ReLU(inplace=True),\n",
    "                                nn.MaxPool1d(kernel_size = (opt.title_seq_len - kernel_size*2 + 2))\n",
    "                            )\n",
    "         for kernel_size in kernel_sizes]\n",
    "\n",
    "        content_convs = [ nn.Sequential(\n",
    "                                nn.Conv1d(in_channels = opt.embedding_dim,\n",
    "                                        out_channels = opt.content_dim,\n",
    "                                        kernel_size = kernel_size),\n",
    "                                nn.BatchNorm1d(opt.content_dim),\n",
    "                                nn.ReLU(inplace=True),\n",
    "\n",
    "                                nn.Conv1d(in_channels = opt.content_dim,\n",
    "                                        out_channels = opt.content_dim,\n",
    "                                        kernel_size = kernel_size),\n",
    "                                nn.BatchNorm1d(opt.content_dim),\n",
    "                                nn.ReLU(inplace=True),\n",
    "                                nn.MaxPool1d(kernel_size = (opt.content_seq_len - kernel_size*2 + 2))\n",
    "                            )\n",
    "            for kernel_size in kernel_sizes ]\n",
    "\n",
    "        self.title_convs = nn.ModuleList(title_convs)\n",
    "        self.content_convs = nn.ModuleList(content_convs)\n",
    "\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(len(kernel_sizes)*(opt.title_dim+opt.content_dim),opt.linear_hidden_size),\n",
    "            nn.BatchNorm1d(opt.linear_hidden_size),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(opt.linear_hidden_size,opt.num_classes),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "        \n",
    "\n",
    "        if opt.embedding_path:\n",
    "            self.encoder.weight.data.copy_(t.from_numpy(np.load(opt.embedding_path)))\n",
    "\n",
    "    def forward(self, title, content):\n",
    "        title = self.encoder(title)\n",
    "        content = self.encoder(content)\n",
    "\n",
    "        if self.opt.static:\n",
    "            title.detach()\n",
    "            content.detach()\n",
    "\n",
    "        title_out = [ title_conv(title.permute(0, 2, 1)) for title_conv in self.title_convs]\n",
    "        content_out = [ content_conv(content.permute(0,2,1)) for content_conv in self.content_convs]\n",
    "        conv_out = t.cat((title_out+content_out),dim=1)\n",
    "        reshaped = conv_out.view(conv_out.size(0), -1)\n",
    "        sigmoid = self.fc((reshaped))\n",
    "        return sigmoid\n",
    "\n",
    "    # def get_optimizer(self):  \n",
    "    #    return  t.optim.Adam([\n",
    "    #             {'params': self.title_conv.parameters()},\n",
    "    #             {'params': self.content_conv.parameters()},\n",
    "    #             {'params': self.fc.parameters()},\n",
    "    #             {'params': self.encoder.parameters(), 'lr': 5e-4}\n",
    "    #         ], lr=self.opt.lr)\n",
    "    # # end method forward"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4.开始跑模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCH = 3           # 训练整批数据多少次,  我只训练了3次\n",
    "BATCH_SIZE = 128\n",
    "LR = 0.002         # 学习率"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/3 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train logloss:  0.43701791763305664\n",
      "valid logloss:  0.46073699566587134\n",
      "train logloss:  0.3912964463233948\n",
      "valid logloss:  0.41645912271947716\n",
      "train logloss:  0.3772851824760437\n",
      "valid logloss:  0.3964012126677\n",
      "train logloss:  0.34570229053497314\n",
      "valid logloss:  0.37854946378487436\n",
      "train logloss:  0.4105028808116913\n",
      "valid logloss:  0.369999797649719\n",
      "train logloss:  0.3139829933643341\n",
      "valid logloss:  0.35444292474781447\n",
      "train logloss:  0.2828434109687805\n",
      "valid logloss:  0.3387564829920405\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 33%|███▎      | 1/3 [01:13<02:26, 73.04s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0 is done\n",
      "train logloss:  0.2947517931461334\n",
      "valid logloss:  0.33039405721066584\n",
      "train logloss:  0.297991544008255\n",
      "valid logloss:  0.32518049921072906\n",
      "train logloss:  0.2922203540802002\n",
      "valid logloss:  0.31840508015611063\n",
      "train logloss:  0.267525315284729\n",
      "valid logloss:  0.31637792525129704\n",
      "train logloss:  0.3234981894493103\n",
      "valid logloss:  0.31094805679129595\n",
      "train logloss:  0.24049006402492523\n",
      "valid logloss:  0.3059733941327387\n",
      "train logloss:  0.23675303161144257\n",
      "valid logloss:  0.29659887421969794\n",
      "train logloss:  0.2970202565193176\n",
      "valid logloss:  0.3003741011547683\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 67%|██████▋   | 2/3 [02:29<01:14, 74.60s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1 is done\n",
      "train logloss:  0.27917879819869995\n",
      "valid logloss:  0.28867411931705234\n",
      "train logloss:  0.22653546929359436\n",
      "valid logloss:  0.29121695237992395\n",
      "train logloss:  0.2424982786178589\n",
      "valid logloss:  0.29961863744198974\n",
      "train logloss:  0.17280668020248413\n",
      "valid logloss:  0.2884653770743902\n",
      "train logloss:  0.2548966705799103\n",
      "valid logloss:  0.28675260619452253\n",
      "train logloss:  0.3268270790576935\n",
      "valid logloss:  0.2830558728108454\n",
      "train logloss:  0.1753063201904297\n",
      "valid logloss:  0.2805648595843483\n",
      "train logloss:  0.2063988298177719\n",
      "valid logloss:  0.27560979521004997\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "100%|██████████| 3/3 [03:45<00:00, 75.06s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 2 is done\n",
      "CPU times: user 2min 44s, sys: 1min 2s, total: 3min 47s\n",
      "Wall time: 3min 52s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "if __name__ == '__main__':\n",
    "    from config import opt\n",
    "    it = 1\n",
    "    opt.content_dim = 100\n",
    "    opt.title_dim = 100\n",
    "    m = MultiCNNTextBNDeep(opt)\n",
    "    if t.cuda.is_available():\n",
    "        m.cuda()\n",
    "    # 数据处理成tensor\n",
    "    label_tensor = torch.from_numpy(np.array(train.label).reshape(-1,1)).float()\n",
    "    title_tensor = t.autograd.Variable(t.from_numpy(np.array(train_q1))).long()\n",
    "    content_tensor = t.autograd.Variable(t.from_numpy(np.array(train_q2))).long()\n",
    "    # train test split\n",
    "    train_idx, test_idx = train_test_split(np.arange(len(train_q1)),test_size=0.2)\n",
    "    train_label_tensor = label_tensor[train_idx]\n",
    "    train_title_tensor = title_tensor[train_idx]\n",
    "    train_content_tensor = content_tensor[train_idx]\n",
    "    valid_label_tensor = label_tensor[test_idx]\n",
    "    valid_title_tensor = title_tensor[test_idx]\n",
    "    valid_content_tensor = content_tensor[test_idx]\n",
    "    del train\n",
    "    del train_q1\n",
    "    del train_q2\n",
    "    del label_tensor\n",
    "    del title_tensor\n",
    "    del content_tensor\n",
    "    #----------------------\n",
    "    # train torch dataset\n",
    "    torch_dataset = Data.TensorDataset(train_title_tensor, train_content_tensor, train_label_tensor)\n",
    "    train_loader = Data.DataLoader(\n",
    "        dataset=torch_dataset,      # torch TensorDataset format\n",
    "        batch_size=BATCH_SIZE,      # mini batch size\n",
    "        shuffle=True,               # random shuffle for training\n",
    "        num_workers=4,              # subprocesses for loading data\n",
    "    )\n",
    "    # valid torch dataset\n",
    "    valid_torch_dataset = Data.TensorDataset(valid_title_tensor, valid_content_tensor, valid_label_tensor)\n",
    "    valid_train_loader = Data.DataLoader(\n",
    "        dataset=valid_torch_dataset,      # torch TensorDataset format\n",
    "        batch_size=128,      # 预测的batch size 可以大一点1024\n",
    "        num_workers=4,              # subprocesses for loading data\n",
    "    )\n",
    "    # optimizer, loss_func\n",
    "    optimizer = torch.optim.Adam(m.parameters(), lr=LR)   # optimize all cnn parameters;Adam比较好用\n",
    "    # loss_func = nn.CrossEntropyLoss()   # the target label is not one-hotted 适用于多分类\n",
    "    loss_func = nn.BCELoss() # binary\n",
    "    loss_func.cuda()\n",
    "    for epoch in tqdm(range(EPOCH)):\n",
    "        for step, (title, content, b_y) in enumerate(train_loader):   # 分配 batch data, normalize x when iterate train_loader\n",
    "            title, content, b_y = title.cuda(), content.cuda(), b_y.cuda()\n",
    "            output = m(title,content)\n",
    "            loss = loss_func(output, b_y)\n",
    "            if it % 200 == 0:\n",
    "                val_loss_list = []\n",
    "                # 写一个验证集的迭代器得到valid logloss\n",
    "                for step, (val_title, val_content, val_b_y) in enumerate(valid_train_loader):   # 分配 batch data, normalize x when iterate train_loader\n",
    "                    val_title, val_content, val_b_y = val_title.cuda(), val_content.cuda(), val_b_y.cuda()\n",
    "                    tmp_m = m(val_title, val_content)\n",
    "                    val_loss_tmp = loss_func(tmp_m, val_b_y) # loss_func可以调整为False，输出的是batch size个logloss而不是平均值\n",
    "                    del tmp_m\n",
    "                    val_loss_list.append(val_loss_tmp.cpu().data.numpy().tolist())\n",
    "                    del val_loss_tmp\n",
    "                loss_print = loss.cpu().data.numpy().tolist()\n",
    "                print('train logloss: ', loss_print)\n",
    "                print('valid logloss: ', np.mean(val_loss_list))\n",
    "                del loss_print\n",
    "                del val_loss_list\n",
    "            optimizer.zero_grad()           # clear gradients for this training step\n",
    "            loss.backward()                 # backpropagation, compute gradients\n",
    "            optimizer.step()                # apply gradients\n",
    "            it += 1\n",
    "        print('epoch %d is done'%epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 保存模型参数\n",
    "torch.save(m.state_dict(), 'save/m1_params.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5.跑测试数据，输出result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_q1 = test.words.apply(fill_unkQ1).tolist()\n",
    "test_q2 = test.words.apply(fill_unkQ2).tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_title_tensor = t.autograd.Variable(t.from_numpy(np.array(test_q1))).long()\n",
    "test_content_tensor = t.autograd.Variable(t.from_numpy(np.array(test_q2))).long()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_gen(title,content):  # 定义batch数据生成器\n",
    "    idx = 0\n",
    "    length = len(title)\n",
    "    while True:\n",
    "        if idx+2000>length:\n",
    "            yield title[idx:idx+2000],content[idx:idx+2000]\n",
    "            break\n",
    "        start = idx\n",
    "        idx += 2000\n",
    "        yield title[start:start+2000],content[start:start+2000]\n",
    "gen = batch_gen(test_title_tensor,test_content_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "87it [00:06, 13.58it/s]\n"
     ]
    }
   ],
   "source": [
    "result = []\n",
    "for title,content in tqdm(gen):\n",
    "    title, content = title.cuda(), content.cuda()\n",
    "    output = m(title,content)\n",
    "    output = list(np.squeeze(output.cpu().data.numpy().tolist()))\n",
    "    result += output\n",
    "    del output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 保存result提交\n",
    "pd.DataFrame(result,columns=['y_pre']).to_csv('save/result.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
