{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/anaconda3/lib/python3.6/site-packages/sklearn/cross_validation.py:41: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7fa108ade750>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.utils.data as Data\n",
    "import torchvision      # 数据库模块\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "# from gensim.models import Word2Vec\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import log_loss\n",
    "torch.manual_seed(1)    # reproducible"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.构建embedding矩阵 train和test要合起来"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "class BOW(object):\n",
    "    def __init__(self, X, min_count=10, maxlen=100):\n",
    "        \"\"\"\n",
    "        X: [[w1, w2],]]\n",
    "        \"\"\"\n",
    "        self.X = X\n",
    "        self.min_count = min_count\n",
    "        self.maxlen = maxlen\n",
    "        self.__word_count()\n",
    "        self.__idx()\n",
    "        self.__doc2num()\n",
    "\n",
    "    def __word_count(self):\n",
    "        wc = {}\n",
    "        for ws in tqdm(self.X, desc='   Word Count'):\n",
    "            for w in ws:\n",
    "                if w in wc:\n",
    "                    wc[w] += 1\n",
    "                else:\n",
    "                    wc[w] = 1\n",
    "        self.word_count = {i: j for i, j in wc.items() if j >= self.min_count}\n",
    "\n",
    "    def __idx(self):\n",
    "        self.idx2word = {i + 1: j for i, j in enumerate(self.word_count)}\n",
    "        self.word2idx = {j: i for i, j in self.idx2word.items()}\n",
    "\n",
    "    def __doc2num(self):\n",
    "        doc2num = []\n",
    "        for text in tqdm(self.X, desc='Doc To Number'):\n",
    "            s = [self.word2idx.get(i, 0) for i in text[:self.maxlen]]\n",
    "            doc2num.append(s + [0]*(self.maxlen-len(s)))  # 未登录词全部用0表示\n",
    "        self.doc2num = np.asarray(doc2num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_ids(qids):\n",
    "    ids = []\n",
    "    for t_ in qids:\n",
    "        ids.append(int(t_[1:]))\n",
    "    return np.asarray(ids)\n",
    "\n",
    "\n",
    "def get_texts(file_path, question_path):\n",
    "    qes = pd.read_csv(question_path)\n",
    "    file = pd.read_csv(file_path)\n",
    "    q1id, q2id = file['q1'], file['q2']\n",
    "    id1s, id2s = get_ids(q1id), get_ids(q2id)\n",
    "    all_words = qes['words']\n",
    "    texts = []\n",
    "    for t_ in zip(id1s, id2s):\n",
    "        texts.append(all_words[t_[0]] + ' ' + all_words[t_[1]])\n",
    "    return texts\n",
    "TRAIN_PATH = 'mojing/train.csv'\n",
    "TEST_PATH = 'mojing/test.csv'\n",
    "QUESTION_PATH = 'mojing/question.csv'\n",
    "train_texts = get_texts(TRAIN_PATH, QUESTION_PATH)\n",
    "test_texts = get_texts(TEST_PATH, QUESTION_PATH)\n",
    "a = train_texts + test_texts\n",
    "a1 = [x.split(' ') for x in a]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "   Word Count: 100%|██████████| 427342/427342 [00:01<00:00, 237201.38it/s]\n",
      "Doc To Number: 100%|██████████| 427342/427342 [00:02<00:00, 151976.34it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 5.37 s, sys: 130 ms, total: 5.5 s\n",
      "Wall time: 5.48 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "bow = BOW(a1,min_count=1,maxlen=24) # count大于1，句子(q1,q2)相加最大长度为24"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2.47 s, sys: 1.69 s, total: 4.16 s\n",
      "Wall time: 5.89 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "word_embed = pd.read_csv('mojing/word_embed.txt',header=None)\n",
    "word_embed.columns = ['wv']\n",
    "word_embed_dict = dict()\n",
    "for s in word_embed.wv.values:\n",
    "    l = s.split(' ')\n",
    "    word_embed_dict[l[0]] = list(map(float,l[1:]))\n",
    "word_embed_dict['UNK'] = [0]*300"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = len(word_embed_dict)\n",
    "embedding_matrix = np.zeros((vocab_size+1,300))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "for key, value in bow.word2idx.items():\n",
    "    embedding_matrix[value] = word_embed_dict.get(key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.        ,  0.        ,  0.        , ...,  0.        ,\n",
       "         0.        ,  0.        ],\n",
       "       [ 2.29952765, -4.29687977,  3.71340919, ...,  0.99011242,\n",
       "         0.41728863,  3.15365911],\n",
       "       [-1.52279055,  2.12538552, -0.3590863 , ..., -2.17771411,\n",
       "         1.37241161, -3.44047666],\n",
       "       ...,\n",
       "       [ 0.        ,  0.        ,  0.        , ...,  0.        ,\n",
       "         0.        ,  0.        ],\n",
       "       [ 0.        ,  0.        ,  0.        , ...,  0.        ,\n",
       "         0.        ,  0.        ],\n",
       "       [ 0.        ,  0.        ,  0.        , ...,  0.        ,\n",
       "         0.        ,  0.        ]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. 导入数据，处理数据"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 拼接数据 将q1和q2问题的词处理，到时候分别输入lstm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "## save\n",
    "# train[['label','words']].to_csv('mojing1/train_words2.csv',index=False)\n",
    "# test[['words']].to_csv('mojing1/test_words2.csv',index=False)\n",
    "# load\n",
    "train = pd.read_csv('mojing1/train_words2.csv')\n",
    "test = pd.read_csv('mojing1/test_words2.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fill_unkQ1(s):\n",
    "    l1 = s.split(',')[0].split(' ')[:12]\n",
    "    l1 += ['UNK']*(12-len(l1))\n",
    "    l1 = [bow.word2idx.get(x) if x in bow.word2idx.keys() else 0 for x in l1]\n",
    "    return l1\n",
    "def fill_unkQ2(s):\n",
    "    l2 = s.split(',')[1].split(' ')[:12]\n",
    "    l2 += ['UNK']*(12-len(l2))\n",
    "    l2 = [bow.word2idx.get(x) if x in bow.word2idx.keys() else 0 for x in l2]\n",
    "    return l2\n",
    "# train = train.words.apply(lambda x : x.split(',')[0].split(' ')[:12]).tolist()\n",
    "# test = test.words.apply(lambda x : x.split(',')[0].split(' ')[:12]).tolist()\n",
    "train_q1 = train.words.apply(fill_unkQ1).tolist()\n",
    "train_q2 = train.words.apply(fill_unkQ2).tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3.构建LSTM模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from BasicModule import BasicModule\n",
    "import torch as t\n",
    "import numpy as np\n",
    "from torch import nn\n",
    "\n",
    "\n",
    "def kmax_pooling(x, dim, k):\n",
    "    index = x.topk(k, dim = dim)[1].sort(dim = dim)[0]\n",
    "    return x.gather(dim, index)\n",
    "\n",
    "class LSTMText(BasicModule): \n",
    "    def __init__(self, opt ):\n",
    "        super(LSTMText, self).__init__()\n",
    "        self.model_name = 'LSTMText'\n",
    "        self.opt=opt\n",
    "\n",
    "        kernel_size = opt.kernel_size\n",
    "        self.encoder = nn.Embedding(opt.vocab_size,opt.embedding_dim)\n",
    "\n",
    "        self.title_lstm = nn.LSTM(input_size = opt.embedding_dim,\\\n",
    "                            hidden_size = opt.hidden_size,\n",
    "                            num_layers = opt.num_layers,\n",
    "                            bias = True,\n",
    "                            batch_first = False,\n",
    "                            dropout = 0.5, # dropout\n",
    "                            bidirectional = True\n",
    "                            )\n",
    "        self.content_lstm =nn.LSTM(input_size = opt.embedding_dim,\\\n",
    "                            hidden_size = opt.hidden_size,\n",
    "                            num_layers = opt.num_layers,\n",
    "                            bias = True,\n",
    "                            batch_first = False,\n",
    "                            dropout = 0.5, # dropout\n",
    "                            bidirectional = True\n",
    "                            )\n",
    "\n",
    "#         self.dropout = nn.Dropout()\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(opt.kmax_pooling*(opt.hidden_size*2*2),opt.linear_hidden_size),\n",
    "            nn.Dropout(0.2), # dropout\n",
    "            nn.BatchNorm1d(opt.linear_hidden_size),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(opt.linear_hidden_size,opt.num_classes),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "        # self.fc = nn.Linear(3 * (opt.title_dim+opt.content_dim), opt.num_classes)\n",
    "        if opt.embedding_path:\n",
    "            self.encoder.weight.data.copy_(t.from_numpy(np.load(opt.embedding_path)))\n",
    " \n",
    "    def forward(self, title, content):\n",
    "        title = self.encoder(title)\n",
    "        content = self.encoder(content)\n",
    "        if self.opt.static:\n",
    "            title=title.detach()\n",
    "            content=content.detach()\n",
    "        \n",
    "        title_out = self.title_lstm(title.permute(1,0,2))[0].permute(1,2,0) \n",
    "\n",
    "        content_out = self.content_lstm(content.permute(1,0,2))[0].permute(1,2,0)\n",
    "\n",
    "\n",
    "        title_conv_out = kmax_pooling((title_out),2,self.opt.kmax_pooling)\n",
    "        content_conv_out = kmax_pooling((content_out),2,self.opt.kmax_pooling)\n",
    "\n",
    "        conv_out = t.cat((title_conv_out,content_conv_out),dim=1)\n",
    "        reshaped = conv_out.view(conv_out.size(0), -1)\n",
    "        sigmoid = self.fc((reshaped))\n",
    "        return sigmoid\n",
    "\n",
    "    # def get_optimizer(self):  \n",
    "    #    return  t.optim.Adam([\n",
    "    #             {'params': self.title_conv.parameters()},\n",
    "    #             {'params': self.content_conv.parameters()},\n",
    "    #             {'params': self.fc.parameters()},\n",
    "    #             {'params': self.encoder.parameters(), 'lr': 5e-4}\n",
    "    #         ], lr=self.opt.lr)\n",
    "    # # end method forward"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4.开始跑模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCH = 5           # 训练整批数据多少次,  我训练了5次\n",
    "BATCH_SIZE = 128\n",
    "LR = 0.002         # 学习率"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/5 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train logloss:  0.5203981399536133\n",
      "valid logloss:  0.4832713657617569\n",
      "train logloss:  0.4543154537677765\n",
      "valid logloss:  0.46938819229602813\n",
      "train logloss:  0.44766563177108765\n",
      "valid logloss:  0.41660228043794634\n",
      "train logloss:  0.41195303201675415\n",
      "valid logloss:  0.40250764340162276\n",
      "train logloss:  0.37728428840637207\n",
      "valid logloss:  0.4089146041870117\n",
      "train logloss:  0.41779762506484985\n",
      "valid logloss:  0.3801661291718483\n",
      "train logloss:  0.3788602948188782\n",
      "valid logloss:  0.38513225942850116\n",
      "train logloss:  0.34949246048927307\n",
      "valid logloss:  0.3846263459324837\n",
      "train logloss:  0.2873624265193939\n",
      "valid logloss:  0.36341639041900636\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 20%|██        | 1/5 [01:49<07:19, 109.79s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0 is done\n",
      "train logloss:  0.3050110340118408\n",
      "valid logloss:  0.3632037815451622\n",
      "train logloss:  0.39709538221359253\n",
      "valid logloss:  0.36454082041978836\n",
      "train logloss:  0.340201735496521\n",
      "valid logloss:  0.3601865091919899\n",
      "train logloss:  0.25215864181518555\n",
      "valid logloss:  0.3529305890202522\n",
      "train logloss:  0.30501216650009155\n",
      "valid logloss:  0.3462480956315994\n",
      "train logloss:  0.31545740365982056\n",
      "valid logloss:  0.34580546751618385\n",
      "train logloss:  0.357025146484375\n",
      "valid logloss:  0.35067665100097656\n",
      "train logloss:  0.2753453552722931\n",
      "valid logloss:  0.33336266726255415\n",
      "train logloss:  0.28542983531951904\n",
      "valid logloss:  0.32995369225740434\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 40%|████      | 2/5 [03:39<05:29, 109.78s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1 is done\n",
      "train logloss:  0.3565730154514313\n",
      "valid logloss:  0.3302577315270901\n",
      "train logloss:  0.31011927127838135\n",
      "valid logloss:  0.3339629453420639\n",
      "train logloss:  0.2464858889579773\n",
      "valid logloss:  0.3411118359863758\n",
      "train logloss:  0.3291729688644409\n",
      "valid logloss:  0.33318966209888456\n",
      "train logloss:  0.3360821604728699\n",
      "valid logloss:  0.3218403685092926\n",
      "train logloss:  0.33644187450408936\n",
      "valid logloss:  0.32533564627170564\n",
      "train logloss:  0.2882845103740692\n",
      "valid logloss:  0.32076655730605125\n",
      "train logloss:  0.3483291566371918\n",
      "valid logloss:  0.3246272438764572\n",
      "train logloss:  0.237955242395401\n",
      "valid logloss:  0.31924026027321817\n",
      "train logloss:  0.3384959101676941\n",
      "valid logloss:  0.31946116253733636\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 60%|██████    | 3/5 [05:31<03:40, 110.44s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 2 is done\n",
      "train logloss:  0.2170129120349884\n",
      "valid logloss:  0.31919210359454153\n",
      "train logloss:  0.23013758659362793\n",
      "valid logloss:  0.3091035702824593\n",
      "train logloss:  0.32513362169265747\n",
      "valid logloss:  0.3210012054443359\n",
      "train logloss:  0.3542996644973755\n",
      "valid logloss:  0.30835054486989977\n",
      "train logloss:  0.20002661645412445\n",
      "valid logloss:  0.30600378423929214\n",
      "train logloss:  0.29175055027008057\n",
      "valid logloss:  0.30958916142582893\n",
      "train logloss:  0.2737938165664673\n",
      "valid logloss:  0.304500747770071\n",
      "train logloss:  0.2920929491519928\n",
      "valid logloss:  0.2969368878006935\n",
      "train logloss:  0.20922386646270752\n",
      "valid logloss:  0.2959528116881847\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 80%|████████  | 4/5 [07:21<01:50, 110.41s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 3 is done\n",
      "train logloss:  0.2762778401374817\n",
      "valid logloss:  0.29851986959576604\n",
      "train logloss:  0.239698588848114\n",
      "valid logloss:  0.29731502816081046\n",
      "train logloss:  0.26651179790496826\n",
      "valid logloss:  0.30353258177638054\n",
      "train logloss:  0.23750382661819458\n",
      "valid logloss:  0.30335391268134115\n",
      "train logloss:  0.24961994588375092\n",
      "valid logloss:  0.30159954965114594\n",
      "train logloss:  0.26876991987228394\n",
      "valid logloss:  0.2965726514160633\n",
      "train logloss:  0.3215482234954834\n",
      "valid logloss:  0.290412195622921\n",
      "train logloss:  0.2949153482913971\n",
      "valid logloss:  0.2922604976594448\n",
      "train logloss:  0.2729884386062622\n",
      "valid logloss:  0.2881654557585716\n",
      "train logloss:  0.22499841451644897\n",
      "valid logloss:  0.2849304476380348\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "100%|██████████| 5/5 [09:14<00:00, 110.80s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 4 is done\n",
      "CPU times: user 5min 27s, sys: 3min 45s, total: 9min 12s\n",
      "Wall time: 9min 21s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "if __name__ == '__main__':\n",
    "    from config import opt\n",
    "    it = 1\n",
    "    opt.content_dim = 100\n",
    "    opt.title_dim = 100\n",
    "    m = LSTMText(opt)\n",
    "    if t.cuda.is_available():\n",
    "        m.cuda()\n",
    "    # 数据处理成tensor\n",
    "    label_tensor = torch.from_numpy(np.array(train.label).reshape(-1,1)).float()\n",
    "    title_tensor = t.autograd.Variable(t.from_numpy(np.array(train_q1))).long()\n",
    "    content_tensor = t.autograd.Variable(t.from_numpy(np.array(train_q2))).long()\n",
    "    # train test split\n",
    "    train_idx, test_idx = train_test_split(np.arange(len(train_q1)),test_size=0.05) # 调节训练集和验证集的比例\n",
    "    train_label_tensor = label_tensor[train_idx]\n",
    "    train_title_tensor = title_tensor[train_idx]\n",
    "    train_content_tensor = content_tensor[train_idx]\n",
    "    valid_label_tensor = label_tensor[test_idx]\n",
    "    valid_title_tensor = title_tensor[test_idx]\n",
    "    valid_content_tensor = content_tensor[test_idx]\n",
    "    del train\n",
    "    del train_q1\n",
    "    del train_q2\n",
    "    del label_tensor\n",
    "    del title_tensor\n",
    "    del content_tensor\n",
    "    #----------------------\n",
    "    # train torch dataset\n",
    "    torch_dataset = Data.TensorDataset(train_title_tensor, train_content_tensor, train_label_tensor)\n",
    "    train_loader = Data.DataLoader(\n",
    "        dataset=torch_dataset,      # torch TensorDataset format\n",
    "        batch_size=BATCH_SIZE,      # mini batch size\n",
    "        shuffle=True,               # random shuffle for training\n",
    "        num_workers=4,              # subprocesses for loading data\n",
    "    )\n",
    "    # valid torch dataset\n",
    "    valid_torch_dataset = Data.TensorDataset(valid_title_tensor, valid_content_tensor, valid_label_tensor)\n",
    "    valid_train_loader = Data.DataLoader(\n",
    "        dataset=valid_torch_dataset,      # torch TensorDataset format\n",
    "        batch_size=128,      # 预测的batch size 可以大一点1024\n",
    "        num_workers=4,              # subprocesses for loading data\n",
    "    )\n",
    "    # optimizer, loss_func\n",
    "    optimizer = torch.optim.Adam(m.parameters(), lr=LR)   # optimize all cnn parameters;Adam比较好用\n",
    "    # loss_func = nn.CrossEntropyLoss()   # the target label is not one-hotted 适用于多分类\n",
    "    loss_func = nn.BCELoss() # binary\n",
    "    loss_func.cuda()\n",
    "    for epoch in tqdm(range(EPOCH)):\n",
    "        for step, (title, content, b_y) in enumerate(train_loader):   # 分配 batch data, normalize x when iterate train_loader\n",
    "            title, content, b_y = title.cuda(), content.cuda(), b_y.cuda()\n",
    "            output = m(title,content)\n",
    "            loss = loss_func(output, b_y)\n",
    "            if it % 200 == 0:\n",
    "                val_loss_list = []\n",
    "                m.eval() # 改成evaluate模型，dropout无效（验证的时候用全模型，不用dropout）\n",
    "                # 写一个验证集的迭代器得到valid logloss\n",
    "                for step, (val_title, val_content, val_b_y) in enumerate(valid_train_loader):   # 分配 batch data, normalize x when iterate train_loader\n",
    "                    val_title, val_content, val_b_y = val_title.cuda(), val_content.cuda(), val_b_y.cuda()\n",
    "                    tmp_m = m(val_title, val_content)\n",
    "                    val_loss_tmp = loss_func(tmp_m, val_b_y) # loss_func可以调整为False，输出的是batch size个logloss而不是平均值\n",
    "                    del tmp_m\n",
    "                    val_loss_list.append(val_loss_tmp.cpu().data.numpy().tolist())\n",
    "                    del val_loss_tmp\n",
    "                loss_print = loss.cpu().data.numpy().tolist()\n",
    "                print('train logloss: ', loss_print)\n",
    "                print('valid logloss: ', np.mean(val_loss_list))\n",
    "                del loss_print\n",
    "                del val_loss_list\n",
    "                m.train() # 变成train模式，dropout有效\n",
    "            optimizer.zero_grad()           # clear gradients for this training step\n",
    "            loss.backward()                 # backpropagation, compute gradients\n",
    "            optimizer.step()                # apply gradients\n",
    "            it += 1\n",
    "        print('epoch %d is done'%epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 保存模型参数\n",
    "torch.save(m.state_dict(), 'save/m2_params.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5.跑测试数据，输出result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_q1 = test.words.apply(fill_unkQ1).tolist()\n",
    "test_q2 = test.words.apply(fill_unkQ2).tolist()\n",
    "test_title_tensor = t.autograd.Variable(t.from_numpy(np.array(test_q1))).long()\n",
    "test_content_tensor = t.autograd.Variable(t.from_numpy(np.array(test_q2))).long()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_gen(title,content):  # 定义batch数据生成器\n",
    "    idx = 0\n",
    "    length = len(title)\n",
    "    while True:\n",
    "        if idx+2000>length:\n",
    "            yield title[idx:idx+2000],content[idx:idx+2000]\n",
    "            break\n",
    "        start = idx\n",
    "        idx += 2000\n",
    "        yield title[start:start+2000],content[start:start+2000]\n",
    "gen = batch_gen(test_title_tensor,test_content_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "87it [00:21,  4.10it/s]\n"
     ]
    }
   ],
   "source": [
    "result = []\n",
    "for title,content in tqdm(gen):\n",
    "    title, content = title.cuda(), content.cuda()\n",
    "    output = m(title,content)\n",
    "    output = list(np.squeeze(output.cpu().data.numpy().tolist()))\n",
    "    result += output\n",
    "    del output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 保存result提交\n",
    "pd.DataFrame(result,columns=['y_pre']).to_csv('save/res2.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
