{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch \n",
    "import torch.nn as nn\n",
    "import torchvision.datasets as dsets\n",
    "import torchvision.transforms as transforms\n",
    "from torch.autograd import Variable\n",
    "import torch.nn.functional as F\n",
    "from config import opt\n",
    "from sklearn.externals import joblib #jbolib模块"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. 数据准备"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1 把数据处理成DSSM要求的格式，一个query，一个pos_doc，四个neg_doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 处理q1,q2\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "train = pd.read_csv('mojing/train.csv')\n",
    "train = train.iloc[:10000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>q1</th>\n",
       "      <th>q2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>Q397345</td>\n",
       "      <td>Q538594</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>Q193805</td>\n",
       "      <td>Q699273</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>Q085471</td>\n",
       "      <td>Q676160</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>Q189314</td>\n",
       "      <td>Q438123</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>Q267714</td>\n",
       "      <td>Q290126</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   label       q1       q2\n",
       "0      1  Q397345  Q538594\n",
       "1      0  Q193805  Q699273\n",
       "2      0  Q085471  Q676160\n",
       "3      0  Q189314  Q438123\n",
       "4      0  Q267714  Q290126"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = train[train.label == 1].copy()\n",
    "a.columns = ['label','query','pos_doc']\n",
    "reorder_col = ['label','pos_doc','query']\n",
    "b = a.loc[:, reorder_col].copy()\n",
    "b.columns = ['label','query','pos_doc']\n",
    "a = a.append(b,ignore_index=True)\n",
    "a.drop_duplicates(subset=['query'],inplace=True)\n",
    "a.index = np.arange(len(a)) # 重排序"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 抽不为1的4个doc (neg)\n",
    "b = train[train.label == 0]\n",
    "suffle_pool = list(b.q1) + list(b.q2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ff(s):\n",
    "    l = []\n",
    "    l += b[b.q1 == s].q2.tolist()\n",
    "    l += b[b.q2 == s].q1.tolist()\n",
    "    l = list(set(l))[:4]\n",
    "    l_ = l.copy()\n",
    "    l_.append(s)\n",
    "    if len(l) < 4:\n",
    "        tmp = np.random.choice(suffle_pool,5,replace=False).tolist()\n",
    "        cha = set(tmp) - set(l_)\n",
    "        l += list(cha)[:4-len(l)]\n",
    "    return l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 27.5 s, sys: 174 ms, total: 27.7 s\n",
      "Wall time: 27.7 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "a['neg_doc'] = a['query'].apply(ff)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>query</th>\n",
       "      <th>pos_doc</th>\n",
       "      <th>neg_doc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>Q397345</td>\n",
       "      <td>Q538594</td>\n",
       "      <td>[Q521609, Q175780, Q068667, Q632305]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>Q369715</td>\n",
       "      <td>Q658908</td>\n",
       "      <td>[Q696189, Q428940, Q198861, Q578218]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>Q537991</td>\n",
       "      <td>Q268444</td>\n",
       "      <td>[Q011513, Q022092, Q229357, Q498790]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>Q639518</td>\n",
       "      <td>Q053248</td>\n",
       "      <td>[Q392805, Q657314, Q647857, Q539673]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>Q683881</td>\n",
       "      <td>Q087150</td>\n",
       "      <td>[Q432305, Q723726, Q272217, Q206480]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   label    query  pos_doc                               neg_doc\n",
       "0      1  Q397345  Q538594  [Q521609, Q175780, Q068667, Q632305]\n",
       "1      1  Q369715  Q658908  [Q696189, Q428940, Q198861, Q578218]\n",
       "2      1  Q537991  Q268444  [Q011513, Q022092, Q229357, Q498790]\n",
       "3      1  Q639518  Q053248  [Q392805, Q657314, Q647857, Q539673]\n",
       "4      1  Q683881  Q087150  [Q432305, Q723726, Q272217, Q206480]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 把question替换成words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2.53 s, sys: 293 ms, total: 2.82 s\n",
      "Wall time: 4.57 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# 把问题替换成词\n",
    "QUESTION_PATH = 'mojing/question.csv'\n",
    "questions = pd.read_csv(QUESTION_PATH)\n",
    "question_dict = {}\n",
    "for key, value in zip(questions['qid'],questions['words']):\n",
    "    question_dict[key] = value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 34.6 ms, sys: 1.96 ms, total: 36.6 ms\n",
      "Wall time: 35.9 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "a['query'] = a['query'].apply(lambda x : question_dict.get(x))\n",
    "a['pos_doc'] = a['pos_doc'].apply(lambda x : question_dict.get(x))\n",
    "a['neg_doc'] = a['neg_doc'].apply(lambda l : [question_dict.get(x) for x in l])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>query</th>\n",
       "      <th>pos_doc</th>\n",
       "      <th>neg_doc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>W04465 W04058 W05284 W02916</td>\n",
       "      <td>W18238 W18843 W01490 W09905</td>\n",
       "      <td>[W06579 W17705 W09745 W10938 W01490 W07863, W1...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>W12908 W19355 W08041 W06040 W18399 W01773 W16319</td>\n",
       "      <td>W12908 W06112 W08041 W17342</td>\n",
       "      <td>[W13157 W16564 W08020 W08924 W08276 W11824 W04...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>W16429 W14586 W03914 W09648 W02262 W18399 W06682</td>\n",
       "      <td>W13522 W05733 W17917 W10691 W16319</td>\n",
       "      <td>[W00022 W06756, W18830 W05733 W08276 W06179 W0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>W04182 W05733 W03914 W09400 W13868</td>\n",
       "      <td>W04476 W11385 W05733 W18804 W16686 W19081 W18448</td>\n",
       "      <td>[W12440 W19536 W17945 W18080 W15175 W19355 W17...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>W17378 W14586 W01661 W03914 W04182 W12803 W02262</td>\n",
       "      <td>W07777 W05733 W04476 W11385 W10628 W08815 W047...</td>\n",
       "      <td>[W18238 W05284 W09158 W04745 W03390, W17378 W0...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   label                                             query  \\\n",
       "0      1                       W04465 W04058 W05284 W02916   \n",
       "1      1  W12908 W19355 W08041 W06040 W18399 W01773 W16319   \n",
       "2      1  W16429 W14586 W03914 W09648 W02262 W18399 W06682   \n",
       "3      1                W04182 W05733 W03914 W09400 W13868   \n",
       "4      1  W17378 W14586 W01661 W03914 W04182 W12803 W02262   \n",
       "\n",
       "                                             pos_doc  \\\n",
       "0                        W18238 W18843 W01490 W09905   \n",
       "1                        W12908 W06112 W08041 W17342   \n",
       "2                 W13522 W05733 W17917 W10691 W16319   \n",
       "3   W04476 W11385 W05733 W18804 W16686 W19081 W18448   \n",
       "4  W07777 W05733 W04476 W11385 W10628 W08815 W047...   \n",
       "\n",
       "                                             neg_doc  \n",
       "0  [W06579 W17705 W09745 W10938 W01490 W07863, W1...  \n",
       "1  [W13157 W16564 W08020 W08924 W08276 W11824 W04...  \n",
       "2  [W00022 W06756, W18830 W05733 W08276 W06179 W0...  \n",
       "3  [W12440 W19536 W17945 W18080 W15175 W19355 W17...  \n",
       "4  [W18238 W05284 W09158 W04745 W03390, W17378 W0...  "
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3 把words进行编码"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "   Word Count: 100%|██████████| 427342/427342 [00:01<00:00, 332864.53it/s]\n",
      "Doc To Number: 100%|██████████| 427342/427342 [00:03<00:00, 133272.96it/s]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "class BOW(object):\n",
    "    def __init__(self, X, min_count=10, maxlen=100):\n",
    "        \"\"\"\n",
    "        X: [[w1, w2],]]\n",
    "        \"\"\"\n",
    "        self.X = X\n",
    "        self.min_count = min_count\n",
    "        self.maxlen = maxlen\n",
    "        self.__word_count()\n",
    "        self.__idx()\n",
    "        self.__doc2num()\n",
    "\n",
    "    def __word_count(self):\n",
    "        wc = {}\n",
    "        for ws in tqdm(self.X, desc='   Word Count'):\n",
    "            for w in ws:\n",
    "                if w in wc:\n",
    "                    wc[w] += 1\n",
    "                else:\n",
    "                    wc[w] = 1\n",
    "        self.word_count = {i: j for i, j in wc.items() if j >= self.min_count}\n",
    "\n",
    "    def __idx(self):\n",
    "        self.idx2word = {i + 1: j for i, j in enumerate(self.word_count)}\n",
    "        self.word2idx = {j: i for i, j in self.idx2word.items()}\n",
    "\n",
    "    def __doc2num(self):\n",
    "        doc2num = []\n",
    "        for text in tqdm(self.X, desc='Doc To Number'):\n",
    "            s = [self.word2idx.get(i, 0) for i in text[:self.maxlen]]\n",
    "            doc2num.append(s + [0]*(self.maxlen-len(s)))  # 未登录词全部用0表示\n",
    "        self.doc2num = np.asarray(doc2num)\n",
    "def get_ids(qids):\n",
    "    ids = []\n",
    "    for t_ in qids:\n",
    "        ids.append(int(t_[1:]))\n",
    "    return np.asarray(ids)\n",
    "\n",
    "\n",
    "def get_texts(file_path, question_path):\n",
    "    qes = pd.read_csv(question_path)\n",
    "    file = pd.read_csv(file_path)\n",
    "    q1id, q2id = file['q1'], file['q2']\n",
    "    id1s, id2s = get_ids(q1id), get_ids(q2id)\n",
    "    all_words = qes['words']\n",
    "    texts = []\n",
    "    for t_ in zip(id1s, id2s):\n",
    "        texts.append(all_words[t_[0]] + ' ' + all_words[t_[1]])\n",
    "    return texts\n",
    "TRAIN_PATH = 'mojing/train.csv'\n",
    "TEST_PATH = 'mojing/test.csv'\n",
    "QUESTION_PATH = 'mojing/question.csv'\n",
    "train_texts = get_texts(TRAIN_PATH, QUESTION_PATH)\n",
    "test_texts = get_texts(TEST_PATH, QUESTION_PATH)\n",
    "a1 = train_texts + test_texts\n",
    "a1 = [x.split(' ') for x in a1]\n",
    "bow = BOW(a1,min_count=1,maxlen=24) # count大于1，句子(q1,q2)相加最大长度为24\n",
    "del a1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.        ,  0.        ,  0.        , ...,  0.        ,\n",
       "         0.        ,  0.        ],\n",
       "       [ 2.29952765, -4.29687977,  3.71340919, ...,  0.99011242,\n",
       "         0.41728863,  3.15365911],\n",
       "       [-1.52279055,  2.12538552, -0.3590863 , ..., -2.17771411,\n",
       "         1.37241161, -3.44047666],\n",
       "       ...,\n",
       "       [ 0.        ,  0.        ,  0.        , ...,  0.        ,\n",
       "         0.        ,  0.        ],\n",
       "       [ 0.        ,  0.        ,  0.        , ...,  0.        ,\n",
       "         0.        ,  0.        ],\n",
       "       [ 0.        ,  0.        ,  0.        , ...,  0.        ,\n",
       "         0.        ,  0.        ]])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "##############################\n",
    "# 之前编码好的已经存成embedding matrix\n",
    "# 可以自己训练\n",
    "# 训练代码\n",
    "word_embed = pd.read_csv('mojing/word_embed.txt',header=None)\n",
    "word_embed.columns = ['wv']\n",
    "word_embed_dict = dict()\n",
    "for s in word_embed.wv.values:\n",
    "    l = s.split(' ')\n",
    "    word_embed_dict[l[0]] = list(map(float,l[1:]))\n",
    "word_embed_dict['UNK'] = [0]*300\n",
    "vocab_size = len(word_embed_dict)\n",
    "embedding_matrix = np.zeros((vocab_size+1,300))\n",
    "for key, value in bow.word2idx.items():\n",
    "    embedding_matrix[value] = word_embed_dict.get(key)\n",
    "embedding_matrix\n",
    "# np.save('save/embedding_matrix.npz',embedding_matrix)\n",
    "# embedding_matrix = np.load('save/embedding_matrix.npz.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fill_unkQ1(s):\n",
    "    l1 = s.split(' ')\n",
    "    l1 = [bow.word2idx.get(x) if x in bow.word2idx.keys() else 0 for x in l1]\n",
    "    return l1\n",
    "def fill_unkQ2(l):\n",
    "    l1 = [[bow.word2idx.get(x) if x in bow.word2idx.keys() else 0 for x in s.split(' ')] for s in l]\n",
    "    return l1\n",
    "a['query'] = a['query'].apply(fill_unkQ1)\n",
    "a['pos_doc'] = a['pos_doc'].apply(fill_unkQ1)\n",
    "a['neg_doc'] = a['neg_doc'].apply(fill_unkQ2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>query</th>\n",
       "      <th>pos_doc</th>\n",
       "      <th>neg_doc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>[1, 2, 3, 4]</td>\n",
       "      <td>[5, 6, 7, 8]</td>\n",
       "      <td>[[723, 1649, 27, 151, 7, 25], [124, 773, 99, 2...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>[45, 21, 46, 47, 48, 49, 30]</td>\n",
       "      <td>[45, 50, 46, 51]</td>\n",
       "      <td>[[39, 324, 837, 66, 287, 238, 1394, 53, 25], [...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>[52, 26, 53, 54, 55, 48, 56]</td>\n",
       "      <td>[57, 58, 59, 60, 30]</td>\n",
       "      <td>[[951, 1383], [2317, 58, 287, 593, 25], [570, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>[71, 58, 53, 72, 73]</td>\n",
       "      <td>[10, 74, 58, 75, 76, 77, 78]</td>\n",
       "      <td>[[333, 116, 594, 764, 698, 21, 613], [5, 28, 6...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>[20, 26, 98, 53, 71, 99, 55]</td>\n",
       "      <td>[100, 58, 10, 74, 101, 102, 103, 48, 104]</td>\n",
       "      <td>[[5, 3, 382, 103, 40], [20, 111, 7, 30, 5, 293...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   label                         query  \\\n",
       "0      1                  [1, 2, 3, 4]   \n",
       "1      1  [45, 21, 46, 47, 48, 49, 30]   \n",
       "2      1  [52, 26, 53, 54, 55, 48, 56]   \n",
       "3      1          [71, 58, 53, 72, 73]   \n",
       "4      1  [20, 26, 98, 53, 71, 99, 55]   \n",
       "\n",
       "                                     pos_doc  \\\n",
       "0                               [5, 6, 7, 8]   \n",
       "1                           [45, 50, 46, 51]   \n",
       "2                       [57, 58, 59, 60, 30]   \n",
       "3               [10, 74, 58, 75, 76, 77, 78]   \n",
       "4  [100, 58, 10, 74, 101, 102, 103, 48, 104]   \n",
       "\n",
       "                                             neg_doc  \n",
       "0  [[723, 1649, 27, 151, 7, 25], [124, 773, 99, 2...  \n",
       "1  [[39, 324, 837, 66, 287, 238, 1394, 53, 25], [...  \n",
       "2  [[951, 1383], [2317, 58, 287, 593, 25], [570, ...  \n",
       "3  [[333, 116, 594, 764, 698, 21, 613], [5, 28, 6...  \n",
       "4  [[5, 3, 382, 103, 40], [20, 111, 7, 30, 5, 293...  "
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.4 数据处理成dssm要求的tensor格式样式"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "J = 4\n",
    "l_Qs = [[] for j in range(len(a))]\n",
    "pos_l_Ds = [[] for j in range(len(a))]\n",
    "# neg_l_Ds = [[] for j in range(J)]\n",
    "neg_l_Ds = np.zeros((J,len(a))).tolist()\n",
    "for i in range(len(a['query'])):\n",
    "    l_Qs[i] = Variable(torch.from_numpy(np.array(a['query'][i]).reshape(1,len(a['query'][i]))).long())\n",
    "    pos_l_Ds[i] = Variable(torch.from_numpy(np.array(a['pos_doc'][i]).reshape(1,len(a['pos_doc'][i]))).long())\n",
    "    for j in range(J):\n",
    "        neg_l_Ds[j][i] = Variable(torch.from_numpy(np.array(a['neg_doc'][i][j]).reshape(1,len(a['neg_doc'][i][j]))).long())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. 构建模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CDSSM(\n",
       "  (query_conv): Conv1d(300, 128, kernel_size=(1,), stride=(1,))\n",
       "  (query_sem): Linear(in_features=128, out_features=64, bias=True)\n",
       "  (doc_conv): Conv1d(300, 128, kernel_size=(1,), stride=(1,))\n",
       "  (doc_sem): Linear(in_features=128, out_features=64, bias=True)\n",
       "  (learn_gamma): Conv1d(1, 1, kernel_size=(1,), stride=(1,))\n",
       "  (encoder): Embedding(20893, 300)\n",
       ")"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from BasicModule import BasicModule\n",
    "import torch as t\n",
    "import numpy as np\n",
    "from torch import nn\n",
    "\n",
    "LETTER_GRAM_SIZE = 1 # See section 3.2. trigram_based word_uni_gram 暂时没用到\n",
    "WINDOW_SIZE = 3 # See section 3.2. 暂时没用到\n",
    "TOTAL_LETTER_GRAMS = opt.vocab_size # Determined from data. See section 3.2. 20893 暂时没用到\n",
    "WORD_DEPTH =300 # See equation (1).  这里我用词向量训练好的embedding 300维\n",
    "K = 128 # Dimensionality of the max-pooling layer. See section 3.4.\n",
    "L = 64 # Dimensionality of latent semantic space. See section 3.5.\n",
    "J = 4 # Number of random unclicked documents serving as negative examples for a query. See section 4.\n",
    "FILTER_LENGTH = 1 # We only consider one time step for convolutions.\n",
    "sample_size = 10000\n",
    "\n",
    "def kmax_pooling(x, dim, k):\n",
    "    index = x.topk(k, dim = dim)[1].sort(dim = dim)[0]\n",
    "    return x.gather(dim, index)\n",
    "\n",
    "class CDSSM(BasicModule):\n",
    "    def __init__(self):\n",
    "        super(CDSSM, self).__init__()\n",
    "        # layers for query\n",
    "        self.query_conv = nn.Conv1d(WORD_DEPTH, K, FILTER_LENGTH)\n",
    "        self.query_sem = nn.Linear(K, L)\n",
    "        # layers for docs\n",
    "        self.doc_conv = nn.Conv1d(WORD_DEPTH, K, FILTER_LENGTH)\n",
    "        self.doc_sem = nn.Linear(K, L)\n",
    "        # learning gamma\n",
    "        self.learn_gamma = nn.Conv1d(1, 1, 1)\n",
    "        # embedding\n",
    "        self.encoder = nn.Embedding(opt.vocab_size,opt.embedding_dim)\n",
    "        if opt.embedding_path:\n",
    "            self.encoder.weight.data.copy_(t.from_numpy(np.load(opt.embedding_path)))\n",
    "    def forward(self, q, pos, negs):\n",
    "        # Query model. The paper uses separate neural nets for queries and documents (see section 5.2).\n",
    "        # To make it compatible with Conv layer we reshape it to: (batch_size, WORD_DEPTH, query_len)\n",
    "#         q = self.encoder(q)\n",
    "#         pos = self.encoder(pos)\n",
    "#         negs = [self.encoder(neg) for neg in negs]\n",
    "        q = q.transpose(1,2)\n",
    "        # In this step, we transform each word vector with WORD_DEPTH dimensions into its\n",
    "        # convolved representation with K dimensions. K is the number of kernels/filters\n",
    "        # being used in the operation. Essentially, the operation is taking the dot product\n",
    "        # of a single weight matrix (W_c) with each of the word vectors (l_t) from the\n",
    "        # query matrix (l_Q), adding a bias vector (b_c), and then applying the tanh activation.\n",
    "        # That is, h_Q = tanh(W_c • l_Q + b_c). Note: the paper does not include bias units.\n",
    "        q_c = F.tanh(self.query_conv(q))\n",
    "        # Next, we apply a max-pooling layer to the convolved query matrix.\n",
    "        q_k = kmax_pooling(q_c, 2, 1)\n",
    "        q_k = q_k.transpose(1,2)\n",
    "        # In this step, we generate the semantic vector represenation of the query. This\n",
    "        # is a standard neural network dense layer, i.e., y = tanh(W_s • v + b_s). Again,\n",
    "        # the paper does not include bias units.\n",
    "        q_s = F.tanh(self.query_sem(q_k))\n",
    "        q_s = q_s.resize(L)\n",
    "        # # The document equivalent of the above query model for positive document\n",
    "        pos = pos.transpose(1,2)\n",
    "        pos_c = F.tanh(self.doc_conv(pos))\n",
    "        pos_k = kmax_pooling(pos_c, 2, 1)\n",
    "        pos_k = pos_k.transpose(1,2)\n",
    "        pos_s = F.tanh(self.doc_sem(pos_k))\n",
    "        pos_s = pos_s.resize(L)\n",
    "        # # The document equivalent of the above query model for negative documents\n",
    "        negs = [neg.transpose(1,2) for neg in negs]\n",
    "        neg_cs = [F.tanh(self.doc_conv(neg)) for neg in negs]\n",
    "        neg_ks = [kmax_pooling(neg_c, 2, 1) for neg_c in neg_cs]\n",
    "        neg_ks = [neg_k.transpose(1,2) for neg_k in neg_ks]\n",
    "        neg_ss = [F.tanh(self.doc_sem(neg_k)) for neg_k in neg_ks]\n",
    "        neg_ss = [neg_s.resize(L) for neg_s in neg_ss]\n",
    "        # Now let us calculates the cosine similarity between the semantic representations of\n",
    "        # a queries and documents\n",
    "        # dots[0] is the dot-product for positive document, this is necessary to remember\n",
    "        # because we set the target label accordingly\n",
    "        dots = [q_s.dot(pos_s)]\n",
    "        dots = dots + [q_s.dot(neg_s) for neg_s in neg_ss]\n",
    "        # dots is a list as of now, lets convert it to torch variable\n",
    "        dots = torch.stack(dots)\n",
    "        # In this step, we multiply each dot product value by gamma. In the paper, gamma is\n",
    "        # described as a smoothing factor for the softmax function, and it's set empirically\n",
    "        # on a held-out data set. We're going to learn gamma's value by pretending it's\n",
    "        # a single 1 x 1 kernel.\n",
    "        with_gamma = self.learn_gamma(dots.resize(J+1, 1, 1))\n",
    "        # Finally, we use the softmax function to calculate P(D+|Q).\n",
    "        prob = F.softmax(with_gamma)\n",
    "        return prob\n",
    "\n",
    "model = CDSSM()\n",
    "model.cuda()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. 跑DSSM模型 SGD 一个个样本跑"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = nn.Embedding(opt.vocab_size,opt.embedding_dim)\n",
    "if opt.embedding_path:\n",
    "            encoder.weight.data.copy_(t.from_numpy(np.load(opt.embedding_path)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# cpu version\n",
    "# Loss and optimizer\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=1e-4, momentum=0.9)\n",
    "\n",
    "# output variable, remember the cosine similarity with positive doc was at 0th index\n",
    "y = np.ndarray(1)\n",
    "# CrossEntropyLoss expects only the index as a long tensor\n",
    "y[0] = 0\n",
    "y = Variable(torch.from_numpy(y).long())\n",
    "\n",
    "for i in range(sample_size):\n",
    "    y_pred = model(l_Qs[i], pos_l_Ds[i], [neg_l_Ds[j][i] for j in range(J)])  \n",
    "    loss = criterion(y_pred.resize(1,J+1), y)\n",
    "    print('%d training loss'%i, loss.data[0])\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[tensor([[ 749,   48,    7,  142,    5,  773]]),\n",
       " tensor([[ 280,  234,  907,  280]]),\n",
       " tensor([[   5,   66,  227,   30,   78]]),\n",
       " tensor([[  58,  234,   90,  493,   16,   35,  267,  268,  493,   13]]),\n",
       " tensor([[  20,    7,  134,   30,  159,   78,   13]]),\n",
       " tensor([[ 235,   95,  257,  301,  260,   84,   30]]),\n",
       " tensor([[   20,  1000,   578,   225]]),\n",
       " tensor([[ 543,  258,  222,   18,  138,  680,  330,   53,   66,  269,\n",
       "           688,  215,   10,  107,   78]]),\n",
       " tensor([[ 175,    7,  555]]),\n",
       " tensor([[   94,    95,    53,    26,    35,  3944,    99,    58]]),\n",
       " tensor([[  10,   94,   95,   21,   58,    3,  113,  109]]),\n",
       " tensor([[  58,  348,  234]]),\n",
       " tensor([[ 58,  88,  90,  84]]),\n",
       " tensor([[ 832,  103,   21,  992]]),\n",
       " tensor([[  51,  158,   17,  603,  232,   20]]),\n",
       " tensor([[ 907,  670,   20,  757,   45,  319]]),\n",
       " tensor([[  20,   21,  235,  305,   98,   53,   35,   10,   55,   48,\n",
       "            56]]),\n",
       " tensor([[   10,    55,    21,   106,   107,  2155,   426,    52,   481]]),\n",
       " tensor([[  20,   33,   57,   58,   35,    5,  261,   30]]),\n",
       " tensor([[  103,  2500,  3197,   350]]),\n",
       " tensor([[  20,   33,  246,  648,  152]]),\n",
       " tensor([[ 3954,     7]]),\n",
       " tensor([[  22,  104,   35,  437,  108,  109]]),\n",
       " tensor([[ 472,   20,   21,  252,    5,  816]]),\n",
       " tensor([[  20,   21,    7,  147]]),\n",
       " tensor([[  58,   40,   35,  560,  150,  108,  109]]),\n",
       " tensor([[ 227,   18,  570,  227]]),\n",
       " tensor([[ 296,    7,   46,  349,  234,   75,   15,  220]]),\n",
       " tensor([[  20,   21,  648,  570,   59,  227,   20,  444,   21,  320,\n",
       "           392,   30]]),\n",
       " tensor([[  103,    21,   530,  1539,    30]]),\n",
       " tensor([[  20,   33,    6,  353,    7,    8]]),\n",
       " tensor([[   9,  467,   30,  287,   58,   25]]),\n",
       " tensor([[  11,  426,  568,   66,  269,   25]]),\n",
       " tensor([[ 379,  299,  181,  287,  108,   25]]),\n",
       " tensor([[   74,   227,    31,   152,    59,  3959]]),\n",
       " tensor([[ 3961,   108,    24,     5,     7]]),\n",
       " tensor([[   20,  1169,   146,     3,    84,   106]]),\n",
       " tensor([[ 106,   22,  400,   75,  267,  436,   25]]),\n",
       " tensor([[  197,   197,   376,   968,  3962,   244,    21]]),\n",
       " tensor([[  10,  478,   40,   41,  106,    3,  107,  108,  109,   13]]),\n",
       " tensor([[   5,    3,  382,  400,   22]]),\n",
       " tensor([[ 235,   95,  379,   12]]),\n",
       " tensor([[  20,  158,   58,   40,   10,   74,  841,  795,   30]]),\n",
       " tensor([[ 1346,    30,   106,     3,   113,    24]]),\n",
       " tensor([[ 31,  59,  39]]),\n",
       " tensor([[   58,  3964,    83,    25,    13]]),\n",
       " tensor([[ 3965,    12,    78,    13]]),\n",
       " tensor([[ 107,  448,    7,  381,  501,   20]]),\n",
       " tensor([[ 207,  293,   22,  498]]),\n",
       " tensor([[ 112,  257,  258,   21]]),\n",
       " tensor([[ 215,   11,   31,   78]]),\n",
       " tensor([[ 124,    6,   20,    7,  555]]),\n",
       " tensor([[ 45,  21,  50,  46]]),\n",
       " tensor([[   15,    81,    20,   198,  3143,    30,    41,   192,   280,\n",
       "           1066]]),\n",
       " tensor([[ 19,   5,  44,  57,  58]]),\n",
       " tensor([[  5,  39,   9]]),\n",
       " tensor([[ 1445,   269,    55,   288,    25]]),\n",
       " tensor([[ 603,  281,  349,  234,   13]]),\n",
       " tensor([[  638,   505,  1856,   235,    95,  1041]]),\n",
       " tensor([[  107,   378,  3547,   104,    25]]),\n",
       " tensor([[  58,   32,  563,  654,  108,  109]]),\n",
       " tensor([[   10,   463,  2534,    30,   343,   413,   291]]),\n",
       " tensor([[ 1203,   207,   312]]),\n",
       " tensor([[  58,   26,  771,  661,   25,   13]]),\n",
       " tensor([[  58,   32,  150,  287,  108,  109]]),\n",
       " tensor([[ 142,   21,  257,    5,  359,  362]]),\n",
       " tensor([[ 472,  975,   25,   13]]),\n",
       " tensor([[  20,   10,   99,  379,   30]])]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "l_Qs[8400:8500]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/fdp/.local/lib/python3.6/site-packages/torch/tensor.py:255: UserWarning: non-inplace resize is deprecated\n",
      "  warnings.warn(\"non-inplace resize is deprecated\")\n",
      "/usr/local/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:85: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "/usr/local/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:17: UserWarning: invalid index of a 0-dim tensor. This will be an error in PyTorch 0.5. Use tensor.item() to convert a 0-dim tensor to a Python number\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 training loss tensor(1.6347)\n",
      "100 training loss tensor(1.6490)\n",
      "200 training loss tensor(1.6017)\n",
      "300 training loss tensor(1.6053)\n",
      "400 training loss tensor(1.6080)\n",
      "500 training loss tensor(1.6090)\n",
      "600 training loss tensor(1.5162)\n",
      "700 training loss tensor(1.6037)\n",
      "800 training loss tensor(1.5920)\n",
      "900 training loss tensor(1.5668)\n",
      "1000 training loss tensor(1.5862)\n",
      "1100 training loss tensor(1.5944)\n",
      "1200 training loss tensor(1.5928)\n",
      "1300 training loss tensor(1.6171)\n",
      "1400 training loss tensor(1.4929)\n",
      "1500 training loss tensor(1.6628)\n",
      "1600 training loss tensor(1.5990)\n",
      "1700 training loss tensor(1.6028)\n",
      "1800 training loss tensor(1.5567)\n",
      "1900 training loss tensor(1.4474)\n",
      "2000 training loss tensor(1.6271)\n",
      "2100 training loss tensor(1.6788)\n",
      "2200 training loss tensor(1.6578)\n",
      "2300 training loss tensor(1.4889)\n",
      "2400 training loss tensor(1.6383)\n",
      "2500 training loss tensor(1.3773)\n",
      "2600 training loss tensor(1.3692)\n",
      "2700 training loss tensor(1.6813)\n",
      "2800 training loss tensor(1.5980)\n",
      "2900 training loss tensor(1.5012)\n",
      "3000 training loss tensor(1.7437)\n",
      "3100 training loss tensor(1.5664)\n",
      "3200 training loss tensor(1.6467)\n",
      "3300 training loss tensor(1.2001)\n",
      "3400 training loss tensor(1.6904)\n",
      "3500 training loss tensor(1.2545)\n",
      "3600 training loss tensor(1.3611)\n",
      "3700 training loss tensor(1.6140)\n",
      "3800 training loss tensor(1.5766)\n",
      "3900 training loss tensor(0.9233)\n",
      "4000 training loss tensor(1.4807)\n",
      "4100 training loss tensor(1.7822)\n",
      "4200 training loss tensor(1.5655)\n",
      "4300 training loss tensor(1.6642)\n",
      "4400 training loss tensor(1.1545)\n",
      "4500 training loss tensor(1.5872)\n",
      "4600 training loss tensor(1.0026)\n",
      "4700 training loss tensor(1.0715)\n",
      "4800 training loss tensor(1.0602)\n",
      "4900 training loss tensor(1.7952)\n",
      "5000 training loss tensor(1.5088)\n",
      "5100 training loss tensor(1.6724)\n",
      "5200 training loss tensor(1.7706)\n",
      "5300 training loss tensor(1.4433)\n",
      "5400 training loss tensor(1.4279)\n",
      "5500 training loss tensor(0.9569)\n",
      "5600 training loss tensor(1.8893)\n",
      "5700 training loss tensor(1.5960)\n",
      "5800 training loss tensor(0.9451)\n",
      "5900 training loss tensor(1.7464)\n",
      "6000 training loss tensor(1.5589)\n",
      "6100 training loss tensor(1.7813)\n",
      "6200 training loss tensor(1.1629)\n",
      "6300 training loss tensor(1.4071)\n",
      "6400 training loss tensor(1.8934)\n",
      "6500 training loss tensor(1.8045)\n",
      "6600 training loss tensor(1.5014)\n",
      "6700 training loss tensor(0.9430)\n",
      "6800 training loss tensor(1.3834)\n",
      "6900 training loss tensor(1.6786)\n",
      "7000 training loss tensor(1.1853)\n",
      "7100 training loss tensor(1.7041)\n",
      "7200 training loss tensor(1.3836)\n",
      "7300 training loss tensor(1.4180)\n",
      "7400 training loss tensor(1.7372)\n",
      "7500 training loss tensor(1.7524)\n",
      "7600 training loss tensor(1.0825)\n",
      "7700 training loss tensor(1.7825)\n",
      "7800 training loss tensor(1.7910)\n",
      "7900 training loss tensor(1.8072)\n",
      "8000 training loss tensor(1.8051)\n",
      "8100 training loss tensor(0.9370)\n",
      "8200 training loss tensor(1.2102)\n",
      "8300 training loss tensor(1.3358)\n",
      "8400 training loss tensor(1.7570)\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0mTraceback (most recent call last)",
      "\u001b[0;32m<timed exec>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# gpu version\n",
    "# Loss and optimizer\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=1e-4, momentum=0.9)\n",
    "criterion.cuda()\n",
    "\n",
    "# output variable, remember the cosine similarity with positive doc was at 0th index\n",
    "y = np.ndarray(1)\n",
    "# CrossEntropyLoss expects only the index as a long tensor\n",
    "y[0] = 0\n",
    "y = Variable(torch.from_numpy(y).long()).cuda()\n",
    "\n",
    "for i in range(sample_size):\n",
    "#     y_pred = model(l_Qs[i], pos_l_Ds[i], [neg_l_Ds[j][i] for j in range(J)])\n",
    "    y_pred = model(encoder(l_Qs[i]).cuda(), encoder(pos_l_Ds[i]).cuda(), [encoder(neg_l_Ds[j][i]).cuda() for j in range(J)])\n",
    "    loss = criterion(y_pred.resize(1,J+1), y)\n",
    "    if i % 100 == 0:\n",
    "        print('%d training loss'%i, loss.cpu().data[0])\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 好像没有收敛。。还有报错"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
